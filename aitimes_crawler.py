import requests
from bs4 import BeautifulSoup
import pandas as pd
import openai
import streamlit as st
import time
import re
import os
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import markdown
from io import BytesIO

class AITimesCrawler:
    def __init__(self):
        self.base_url = "https://www.aitimes.com"
        self.main_url = "https://www.aitimes.com/"
        
    def crawl_news_list(self):
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ìƒìœ„ 10ê°œ ë‰´ìŠ¤ì˜ ì œëª©ê³¼ URLì„ í¬ë¡¤ë§"""
        try:
            crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(self.main_url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‰´ìŠ¤ ë§í¬ë“¤ì„ ì°¾ê¸°
            news_links = soup.find_all('a', class_='auto-valign')
            
            news_data = []
            
            for i, link in enumerate(news_links[:10], 1):
                try:
                    # ìˆœìœ„ ë²ˆí˜¸ ì¶”ì¶œ
                    number_elem = link.find('em', class_='number')
                    if number_elem:
                        rank = number_elem.get_text(strip=True)
                    else:
                        rank = str(i)
                    
                    # ì œëª© ì¶”ì¶œ
                    title_elem = link.find('span', class_='auto-titles')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    else:
                        continue
                    
                    # URL ì¶”ì¶œ
                    href = link.get('href')
                    if href:
                        if href.startswith('/'):
                            full_url = self.base_url + href
                        else:
                            full_url = href
                    else:
                        continue
                    
                    news_data.append({
                        'rank': rank,
                        'title': title,
                        'url': full_url,
                        'crawl_time': crawl_time
                    })
                    
                except Exception as e:
                    st.warning(f"ë‰´ìŠ¤ {i}ë²ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            return news_data
            
        except Exception as e:
            st.error(f"ë‰´ìŠ¤ ëª©ë¡ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def crawl_article_content(self, url):
        """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë³¸ë¬¸ì„ í¬ë¡¤ë§"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒì ì‹œë„)
            content_selectors = [
                'div.news-content',
                'div.article-content',
                'div.view-content',
                'div#article-content',
                'div.content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    # p íƒœê·¸ë“¤ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    paragraphs = content_div.find_all('p')
                    content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                    break
            
            # ìœ„ ë°©ë²•ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°, ëª¨ë“  p íƒœê·¸ ì‹œë„
            if not content:
                paragraphs = soup.find_all('p')
                content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            
            return content.strip()
            
        except Exception as e:
            st.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return ""
    
    def summarize_with_gpt(self, title, content, api_key):
        """OpenAI GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ë¥¼ ìš”ì•½"""
        try:
            from openai import OpenAI
            client = OpenAI(api_key=api_key)
            
            prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì•„ë˜ í˜•ì‹ì— ë§ì¶° í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ê¸°ì‚¬ ì œëª©: {title}
ê¸°ì‚¬ ë‚´ìš©: {content}

---
## ğŸš€ {title}

### ğŸ’¡ í•µì‹¬ ë¹„ìœ  (Analogy)
- ë‚´ìš©ì„ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•˜ê³  ê¸°ì–µí•˜ê¸° ì‰¬ìš´ ë¹„ìœ  ë˜ëŠ” ìºì¹˜í”„ë ˆì´ì¦ˆ

### âœ¨ í•µì‹¬ ìš”ì•½ (Key Points)
- ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš© 3ê°€ì§€ ìš”ì•½
    - Point 1
    - Point 2
    - Point 3

### ğŸ“š ìƒì„¸ ë‚´ìš© (Details)
- í•µì‹¬ ìš”ì•½ì—ì„œ ì œì‹œëœ ë‚´ìš©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª…, ë°°ê²½ ë˜ëŠ” ì£¼ìš” íŠ¹ì§• ê¸°ìˆ 

### ğŸ¤” ë¹„íŒì  ê´€ì  (Critical Points)
- í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ ì£¼ì˜ ê¹Šê²Œ ìƒê°í•˜ê±°ë‚˜ ê²½ê³„í•´ì•¼ í•  ì§€ì , ë˜ëŠ” ë” ê¹Šì´ ìƒê°í•´ ë³¼ ë§Œí•œ ì§ˆë¬¸ ì œì‹œ
    - Point 1
    - Point 2

### ğŸ“Š ìˆ«ì (Numbers)
*(ì„ íƒ ì‚¬í•­: ê´€ë ¨ ë°ì´í„°ê°€ ì¤‘ìš”í•  ê²½ìš°)*
- í•µì‹¬ í†µê³„ 1:
- í•µì‹¬ í†µê³„ 2:

### ğŸ‘Ÿ ì‰¬ìš´ ì²«ê±¸ìŒ (Easy Next Step)
- í•µì‹¬ êµí›ˆì„ ë°”íƒ•ìœ¼ë¡œ, ê°€ì¥ ë§ˆì°°ì´ ì ê³  ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ í–‰ë™ 1ê°€ì§€ ì œì•ˆ

---

### ğŸ§© í•µì‹¬ ê°œë… & ìš©ì–´
- ê¸°ìˆ ì ìœ¼ë¡œ ì¤‘ìš”í•˜ê±°ë‚˜ ì–´ë ¤ìš´ í•µì‹¬ ìš©ì–´ 3ê°œë¥¼ ë¹„ìœ ë¥¼ í†µí•´ í•œ ì¤„ë¡œ ì„¤ëª…í•˜ì—¬ ì†Œí™” ë° ê¸°ì–µì„ ë•ìŠµë‹ˆë‹¤.
    - **ìš©ì–´ 1**:
    - **ìš©ì–´ 2**:
    - **ìš©ì–´ 3**:

### ğŸ“– ì°¸ê³ : ì„ í–‰ ì§€ì‹ (Prerequisites)
- ì´ ì •ë³´ë¥¼ ì™„ì „íˆ ì´í•´í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì‚¬ì „ ì§€ì‹ì´ë‚˜ ì¡°ê±´
"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",  # gpt-4.1-miniê°€ ì•„ì§ ì—†ìœ¼ë¯€ë¡œ gpt-4o-mini ì‚¬ìš©
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            st.error(f"AI ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
    
    def save_to_csv(self, news_data):
        """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # crawled_data ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs("crawled_data", exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„± (aitimes_yyyy_mm_dd_hhmmss.csv)
            timestamp = datetime.now().strftime("%Y_%m_%d_%H%M%S")
            filename = f"crawled_data/aitimes_{timestamp}.csv"
            
            df = pd.DataFrame(news_data)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            return filename
        except Exception as e:
            st.error(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def create_pdf_report(self, csv_file_path):
        """CSV íŒŒì¼ì„ ì½ì–´ì„œ PDF ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(csv_file_path)
            
            # PDF íŒŒì¼ëª… ìƒì„±
            csv_filename = os.path.basename(csv_file_path)
            pdf_filename = csv_filename.replace('.csv', '_report.pdf')
            pdf_path = os.path.join(os.path.dirname(csv_file_path), pdf_filename)
            
            # í•œê¸€ í°íŠ¸ ë“±ë¡ (CID í°íŠ¸ ì‚¬ìš©)
            try:
                from reportlab.pdfbase.cidfonts import UnicodeCIDFont
                
                # í•œê¸€ ì§€ì› CID í°íŠ¸ ë“±ë¡ ì‹œë„
                korean_fonts = ['HYSMyeongJoStd-Medium', 'HYGothic-Medium', 'AppleGothic']
                korean_font_registered = False
                
                for font_name in korean_fonts:
                    try:
                        pdfmetrics.registerFont(UnicodeCIDFont(font_name))
                        korean_font_registered = True
                        st.info(f"âœ… í•œê¸€ CID í°íŠ¸ ë¡œë“œ ì„±ê³µ: {font_name}")
                        break
                    except:
                        continue
                
                if not korean_font_registered:
                    st.info("â„¹ï¸ CID í°íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    
            except Exception as font_error:
                st.info(f"â„¹ï¸ CID í°íŠ¸ ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {str(font_error)}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # PDF ë¬¸ì„œ ìƒì„±
            doc = SimpleDocTemplate(pdf_path, pagesize=A4,
                                  rightMargin=inch, leftMargin=inch,
                                  topMargin=inch, bottomMargin=inch)
            
            # ìŠ¤íƒ€ì¼ ì„¤ì •
            styles = getSampleStyleSheet()
            
            # ì‚¬ìš©í•  í°íŠ¸ ê²°ì • (CID í°íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸)
            korean_fonts = ['HYSMyeongJoStd-Medium', 'HYGothic-Medium', 'AppleGothic']
            available_korean_font = None
            
            for font_name in korean_fonts:
                try:
                    # í°íŠ¸ê°€ ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    from reportlab.pdfbase.pdfmetrics import getFont
                    getFont(font_name)
                    available_korean_font = font_name
                    break
                except:
                    continue
            
            regular_font = available_korean_font if available_korean_font else 'Helvetica'
            bold_font = available_korean_font if available_korean_font else 'Helvetica-Bold'
            
            # ì œëª© ìŠ¤íƒ€ì¼
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=24,
                spaceAfter=30,
                alignment=1,  # ì¤‘ì•™ ì •ë ¬
                textColor='#2E86AB',
                fontName=bold_font
            )
            
            # í—¤ë”© ìŠ¤íƒ€ì¼
            heading_style = ParagraphStyle(
                'CustomHeading',
                parent=styles['Heading2'],
                fontSize=16,
                spaceAfter=12,
                spaceBefore=20,
                textColor='#A23B72',
                fontName=bold_font
            )
            
            # ë³¸ë¬¸ ìŠ¤íƒ€ì¼
            body_style = ParagraphStyle(
                'CustomBody',
                parent=styles['Normal'],
                fontSize=11,
                spaceAfter=12,
                leading=14,
                fontName=regular_font
            )
            
            # ì†Œì œëª© ìŠ¤íƒ€ì¼
            subtitle_style = ParagraphStyle(
                'CustomSubtitle',
                parent=styles['Normal'],
                fontSize=13,
                spaceAfter=8,
                spaceBefore=12,
                textColor='#F18F01',
                fontName=bold_font
            )
            
            # ë¦¬í¬íŠ¸ ë‚´ìš© êµ¬ì„±
            story = []
            
            # ì œëª©
            crawl_time = df['crawl_time'].iloc[0] if 'crawl_time' in df.columns else "Unknown"
            story.append(Paragraph(f"AIíƒ€ì„ìŠ¤ ë‰´ìŠ¤ ë¦¬í¬íŠ¸", title_style))
            story.append(Paragraph(f"í¬ë¡¤ë§ ì‹œê°„: {crawl_time}", body_style))
            story.append(Spacer(1, 20))
            
            # ìš”ì•½ í†µê³„
            story.append(Paragraph("ğŸ“Š í¬ë¡¤ë§ ìš”ì•½", heading_style))
            story.append(Paragraph(f"â€¢ ì´ ë‰´ìŠ¤ ê°œìˆ˜: {len(df)}ê°œ", body_style))
            
            successful_summaries = len(df[~df['summary'].str.startswith('ìš”ì•½ ì‹¤íŒ¨', na=False)]) if 'summary' in df.columns else 0
            story.append(Paragraph(f"â€¢ AI ìš”ì•½ ì„±ê³µ: {successful_summaries}ê°œ", body_style))
            story.append(Spacer(1, 20))
            
            # ê° ë‰´ìŠ¤ë³„ ìƒì„¸ ë¦¬í¬íŠ¸
            for idx, row in df.iterrows():
                # í˜ì´ì§€ ë‚˜ëˆ„ê¸° (ì²« ë²ˆì§¸ ë‰´ìŠ¤ ì œì™¸)
                if idx > 0:
                    story.append(PageBreak())
                
                # ë‰´ìŠ¤ ì œëª©
                story.append(Paragraph(f"ğŸ“° ë‰´ìŠ¤ #{row.get('rank', idx+1)}: {row['title']}", heading_style))
                story.append(Paragraph(f"URL: {row['url']}", body_style))
                story.append(Spacer(1, 12))
                
                # AI ìš”ì•½ì´ ìˆëŠ” ê²½ìš°
                if 'summary' in row and pd.notna(row['summary']) and not str(row['summary']).startswith('ìš”ì•½ ì‹¤íŒ¨'):
                    summary_text = str(row['summary'])
                    
                    # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ë³€í™˜ í›„ ë‹¨ìˆœí™”
                    try:
                        # ë§ˆí¬ë‹¤ìš´ íŠ¹ìˆ˜ ë¬¸ìë“¤ì„ ë‹¨ìˆœí™”
                        summary_text = summary_text.replace('### ', '')
                        summary_text = summary_text.replace('## ', '')
                        summary_text = summary_text.replace('# ', '')
                        summary_text = summary_text.replace('**', '')
                        summary_text = summary_text.replace('*', 'â€¢')
                        summary_text = summary_text.replace('- ', 'â€¢ ')
                        
                        # ì´ëª¨ì§€ì™€ ì œëª©ë“¤ì„ ì ì ˆíˆ ì²˜ë¦¬
                        lines = summary_text.split('\n')
                        processed_lines = []
                        
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                                
                            # ì´ëª¨ì§€ê°€ í¬í•¨ëœ ì„¹ì…˜ ì œëª©ë“¤
                            if any(emoji in line for emoji in ['ğŸš€', 'ğŸ’¡', 'âœ¨', 'ğŸ“š', 'ğŸ¤”', 'ğŸ“Š', 'ğŸ‘Ÿ', 'ğŸ§©', 'ğŸ“–']):
                                processed_lines.append(f"<b>{line}</b>")
                            else:
                                processed_lines.append(line)
                        
                        summary_text = '<br/>'.join(processed_lines)
                        story.append(Paragraph(summary_text, body_style))
                        
                    except Exception as e:
                        # ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                        clean_text = summary_text.replace('<', '&lt;').replace('>', '&gt;')
                        story.append(Paragraph(clean_text, body_style))
                        
                else:
                    story.append(Paragraph("AI ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.", body_style))
                
                story.append(Spacer(1, 20))
            
            # PDF ìƒì„±
            doc.build(story)
            return pdf_path
            
        except Exception as e:
            st.error(f"PDF ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def get_csv_files(self):
        """crawled_data ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            if not os.path.exists("crawled_data"):
                return []
            
            csv_files = []
            for file in os.listdir("crawled_data"):
                if file.endswith('.csv') and file.startswith('aitimes_'):
                    csv_files.append(os.path.join("crawled_data", file))
            
            # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
            csv_files.sort(key=os.path.getmtime, reverse=True)
            return csv_files
            
        except Exception as e:
            st.error(f"CSV íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return [] 