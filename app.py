import streamlit as st
import pandas as pd
import time
from aitimes_crawler import AITimesCrawler
import os

def main():
    st.set_page_config(
        page_title="AIíƒ€ì„ìŠ¤ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
        page_icon="ğŸ“°",
        layout="wide"
    )
    
    st.title("ğŸ“° AIíƒ€ì„ìŠ¤ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
    st.markdown("AIíƒ€ì„ìŠ¤ì—ì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  AIë¡œ ìš”ì•½í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ì„¤ì •")
    
    # OpenAI API í‚¤ ì…ë ¥
    api_key = st.sidebar.text_input(
        "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        type="password",
        help="https://platform.openai.com/api-keys ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = AITimesCrawler()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§")
        
        if st.button("AIíƒ€ì„ìŠ¤ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘", type="primary"):
            with st.spinner("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                news_list = crawler.crawl_news_list()
                
                if news_list:
                    st.success(f"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                    
                    # ë‰´ìŠ¤ ëª©ë¡ í‘œì‹œ
                    df_preview = pd.DataFrame(news_list)
                    st.dataframe(df_preview, use_container_width=True)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.news_list = news_list
                    st.session_state.df_preview = df_preview
                    
                else:
                    st.error("âŒ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    with col2:
        st.subheader("ğŸ¤– AI ìš”ì•½ ìƒì„±")
        
        if st.button("ë³¸ë¬¸ í¬ë¡¤ë§ ë° AI ìš”ì•½", type="secondary"):
            if not api_key:
                st.error("âŒ OpenAI API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!")
                return
            
            if 'news_list' not in st.session_state:
                st.error("âŒ ë¨¼ì € ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”!")
                return
            
            news_list = st.session_state.news_list
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            enhanced_news = []
            
            for i, news in enumerate(news_list):
                status_text.text(f"ğŸ“„ {i+1}/{len(news_list)}: {news['title'][:50]}... ì²˜ë¦¬ ì¤‘")
                
                # ë³¸ë¬¸ í¬ë¡¤ë§
                content = crawler.crawl_article_content(news['url'])
                
                if content:
                    # AI ìš”ì•½
                    status_text.text(f"ğŸ¤– {i+1}/{len(news_list)}: AI ìš”ì•½ ìƒì„± ì¤‘...")
                    summary = crawler.summarize_with_gpt(news['title'], content, api_key)
                    
                    enhanced_news.append({
                        'rank': news['rank'],
                        'title': news['title'],
                        'url': news['url'],
                        'content': content,
                        'summary': summary,
                        'crawl_time': news['crawl_time']
                    })
                else:
                    enhanced_news.append({
                        'rank': news['rank'],
                        'title': news['title'],
                        'url': news['url'],
                        'content': "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        'summary': "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        'crawl_time': news['crawl_time']
                    })
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress((i + 1) / len(news_list))
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§€ì—°
                time.sleep(1)
            
            # CSV ì €ì¥
            csv_file = crawler.save_to_csv(enhanced_news)
            if csv_file:
                st.success(f"âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! íŒŒì¼: {csv_file}")
                
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.enhanced_news = enhanced_news
                st.session_state.csv_file = csv_file
            
            status_text.text("âœ… ì™„ë£Œ!")
    
    # ê²°ê³¼ í‘œì‹œ
    if 'enhanced_news' in st.session_state:
        st.markdown("---")
        st.subheader("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ")
        
        enhanced_news = st.session_state.enhanced_news
        
        # í†µê³„ ì •ë³´
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì „ì²´ ë‰´ìŠ¤", len(enhanced_news))
        with col2:
            successful_crawls = len([n for n in enhanced_news if n['content'] != "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."])
            st.metric("ë³¸ë¬¸ í¬ë¡¤ë§ ì„±ê³µ", successful_crawls)
        with col3:
            successful_summaries = len([n for n in enhanced_news if not n['summary'].startswith("ìš”ì•½ ì‹¤íŒ¨")])
            st.metric("AI ìš”ì•½ ì„±ê³µ", successful_summaries)
        
        # ë‰´ìŠ¤ ì„ íƒ ë° ìƒì„¸ ë³´ê¸°
        st.subheader("ğŸ“° ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸°")
        
        # ë‰´ìŠ¤ ì„ íƒ ë“œë¡­ë‹¤ìš´
        news_titles = [f"{news['rank']}. {news['title']}" for news in enhanced_news]
        selected_news_index = st.selectbox(
            "ë³´ê³  ì‹¶ì€ ë‰´ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
            range(len(news_titles)),
            format_func=lambda x: news_titles[x]
        )
        
        selected_news = enhanced_news[selected_news_index]
        
        # ì„ íƒëœ ë‰´ìŠ¤ í‘œì‹œ
        st.markdown(f"### ğŸ“– {selected_news['title']}")
        st.markdown(f"**ìˆœìœ„:** {selected_news['rank']}")
        st.markdown(f"**URL:** {selected_news['url']}")
        
        # íƒ­ìœ¼ë¡œ ì›ë¬¸ê³¼ ìš”ì•½ ë¶„ë¦¬
        tab1, tab2 = st.tabs(["ğŸ¤– AI ìš”ì•½", "ğŸ“„ ì›ë¬¸"])
        
        with tab1:
            if selected_news['summary'] and not selected_news['summary'].startswith("ìš”ì•½ ì‹¤íŒ¨"):
                st.markdown(selected_news['summary'])
            else:
                st.error("AI ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
        
        with tab2:
            if selected_news['content'] and selected_news['content'] != "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                st.text_area("ì›ë¬¸ ë‚´ìš©", selected_news['content'], height=400)
            else:
                st.error("ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
        
        # CSV ë‹¤ìš´ë¡œë“œ ë° PDF ë¦¬í¬íŠ¸
        if 'csv_file' in st.session_state:
            st.markdown("---")
            st.subheader("ğŸ’¾ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                try:
                    with open(st.session_state.csv_file, 'rb') as file:
                        st.download_button(
                            label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                            data=file.read(),
                            file_name=os.path.basename(st.session_state.csv_file),
                            mime='text/csv'
                        )
                except Exception as e:
                    st.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            with col2:
                if st.button("ğŸ“„ PDF ë¦¬í¬íŠ¸ ìƒì„±", type="secondary"):
                    with st.spinner("PDF ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                        pdf_path = crawler.create_pdf_report(st.session_state.csv_file)
                        
                        if pdf_path and os.path.exists(pdf_path):
                            st.success("âœ… PDF ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            
                            try:
                                with open(pdf_path, 'rb') as pdf_file:
                                    st.download_button(
                                        label="ğŸ“¥ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                                        data=pdf_file.read(),
                                        file_name=os.path.basename(pdf_path),
                                        mime='application/pdf'
                                    )
                            except Exception as e:
                                st.error(f"PDF ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        else:
                            st.error("âŒ PDF ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # ê¸°ì¡´ CSV íŒŒì¼ë¡œ PDF ìƒì„±
    st.markdown("---")
    st.subheader("ğŸ“‚ ê¸°ì¡´ ë°ì´í„°ë¡œ PDF ë¦¬í¬íŠ¸ ìƒì„±")
    
    csv_files = crawler.get_csv_files()
    if csv_files:
        selected_csv = st.selectbox(
            "PDFë¡œ ë³€í™˜í•  CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:",
            csv_files,
            format_func=lambda x: os.path.basename(x)
        )
        
        if st.button("ğŸ“„ ì„ íƒí•œ íŒŒì¼ë¡œ PDF ìƒì„±"):
            with st.spinner("PDF ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                pdf_path = crawler.create_pdf_report(selected_csv)
                
                if pdf_path and os.path.exists(pdf_path):
                    st.success("âœ… PDF ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    try:
                        with open(pdf_path, 'rb') as pdf_file:
                            st.download_button(
                                label="ğŸ“¥ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                                data=pdf_file.read(),
                                file_name=os.path.basename(pdf_path),
                                mime='application/pdf',
                                key="existing_csv_pdf"
                            )
                    except Exception as e:
                        st.error(f"PDF ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                else:
                    st.error("âŒ PDF ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ“‚ crawled_data í´ë”ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 